{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#parser\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#NLP\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# import spacy\n",
    "from nltk import pos_tag \n",
    "# Lemmatize using WordNet’s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mjjackey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mjjackey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mjjackey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\mjjackey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mjjackey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"maxent_treebank_pos_tagger\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\")) #stopwords\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "tokenizer=TweetTokenizer() #tokenizer\n",
    "lem = WordNetLemmatizer()  #stem,Lemmatize using WordNet’s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\Jupyter Notebook\\COMP6237_Data_Mining\\output3\n"
     ]
    }
   ],
   "source": [
    "# 有效代码\n",
    "cwd=os.getcwd()\n",
    "fd_str=os.path.join(cwd,\"output3\")\n",
    "print(fd_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效代码\n",
    "def read_file(file_path):\n",
    "#     with open(file_path, 'rt',encoding='latin-1') as f:\n",
    "    with open(file_path, 'rt',encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效代码\n",
    "def write_file(file_path,text):\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效代码\n",
    "doc_list=[]\n",
    "filenames=[]\n",
    "for name in os.listdir(fd_str):\n",
    "    if name.endswith('.txt') and os.path.isfile(os.path.join(fd_str, name)):\n",
    "        filenames.append(name)\n",
    "        file_path=os.path.join(fd_str, name)\n",
    "        text=read_file(file_path) \n",
    "        doc_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 gap_2X5KAAAAYAAJ.txt\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_list),filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430085\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_ascii(doc_text):\n",
    "    doc_text = unicodedata.normalize('NFD', str(doc_text))\n",
    "    doc_text=doc_text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "doc_text_list is list，只保留空格和字母，适合没有分词前对text做pre-clean\n",
    "'''\n",
    "def retain_letter_space(doc_text_list):\n",
    "    doc_text=''.join(e for e in doc_text_list if e.isalpha() or e.isspace())\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "doc_text_list is list\n",
    "'''\n",
    "def retain_letter_space1(doc_text_list):\n",
    "    doc_text=' '.join(e for e in doc_text_list if e.isalpha() or e.isspace())\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "doc_text_list is list，保留了单个字母（缩写含.）的人名的given name\n",
    "'''\n",
    "def retain_letter2(doc_text_list):\n",
    "    doc_text=' '.join(e for e in doc_text_list if len(e)==2 and e[0].isalpha() and e[1]=='.' or e.isalpha() and len(e)>=2) \n",
    "#     doc_text=''\n",
    "#     for e in doc_text_list:\n",
    "# #         print(e)\n",
    "#         if len(e)==2 and e[0].isalpha() or e.isalpha() or e.isspace():\n",
    "# #             print('clean:'+e)\n",
    "#             doc_text=doc_text+' '+e\n",
    "# #             print('doc_text:'+doc_text)\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "doc_text_list is list，去除单个字母（缩写含.）的人名的given name\n",
    "'''\n",
    "def retain_letter3(doc_text_list):\n",
    "    doc_text=' '.join(e for e in doc_text_list if e.isalpha() and len(e)>=2) \n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_romanumber(s):\n",
    "#     remap={\n",
    "#         ord('i'):'',\n",
    "#         str('ii'):'',\n",
    "#        ord('iii'):'',\n",
    "#         ord('iv'):'',\n",
    "#         ord('v'):'',\n",
    "#         ord('vi'):'',\n",
    "#         ord('vii'):'',\n",
    "#         ord('viii'):'',\n",
    "#         ord('ix'):'',\n",
    "#         ord('x'):'',\n",
    "#     }\n",
    "\n",
    "#     s=s.replace('viii','')\n",
    "#     s=s.replace('vii','')\n",
    "#     s=s.replace('vi','')\n",
    "#     s=s.replace('v','')\n",
    "#     s=s.replace('iv','')\n",
    "#     s=s.replace('iii','')\n",
    "#     s=s.replace('ix','')\n",
    "#     s=s.replace('x','')\n",
    "#     s=s.replace('ii','')\n",
    "#     s=s.replace('i','')\n",
    "\n",
    "#     regex=r'^i$|^ii$|^iii$|^iv$|^v$|^vi$|^vii$|^viii$|^ix$|^x$'\n",
    "    regex=r'i|ii|iii|iv|v|vi|vii|viii|ix|x'\n",
    "    s=re.sub(regex,'',s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "不能去除特殊符号，慢\n",
    "'''\n",
    "def clean_punctuation(doc_text):\n",
    "    remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    return doc_text.translate(remove_punct_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "可以去除特殊符号，快\n",
    "'''\n",
    "def clean_punctuation2(doc_text):\n",
    "    tbl=dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "    return doc_text.translate(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "标注词形\n",
    "'''\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean(doc_text):\n",
    "    #remove 'OCR Output'\n",
    "    doc_text=doc_text.replace('OCR Output','')\n",
    "    #convert to lower case\n",
    "    doc_text=doc_text.lower()\n",
    "    #just retain the letter and the space char,简写的.会去掉,比如人名的given name\n",
    "#     doc_text=retain_letter_space(doc_text)\n",
    "    #remove roma number(3-10)\n",
    "#     doc_text=clean_romanumber(doc_text)\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean2(doc_text):\n",
    "    #remove 'OCR Output'\n",
    "    doc_text=doc_text.replace('OCR Output','')\n",
    "    #convert to lower case\n",
    "    doc_text=doc_text.lower()\n",
    "    #just retain the letter and the space char,简写的.会去掉,比如人名的given name\n",
    "    doc_text=retain_letter_space(doc_text)\n",
    "    #remove roma number(3-10)\n",
    "#     doc_text=clean_romanumber(doc_text)\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "按空格和标点符号分词；\n",
    "分词后去除单个字母；\n",
    "使用WordNetLemmatizer，默认词性是verb\n",
    "'''\n",
    "def clean1(text):\n",
    "    #1.Pre-clean\n",
    "    text=preclean(text)\n",
    "    #2.toknenize\n",
    "    text_list=tokenizer.tokenize(text)  #按空格和标点符号分\n",
    "    #remove one letter\n",
    "    text_list = [w for w in text_list if len(w)>=2]\n",
    "    #3.remove stopwords\n",
    "    text_list = [w for w in text_list if not w in eng_stopwords] \n",
    "    #4.lemmatize,stem\n",
    "    text_list = [lem.lemmatize(word, \"v\") for word in text_list] \n",
    "    text = retain_letter_space1(text_list)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "先按句分，再按词分；\n",
    "分词后去除单个字母；\n",
    "使用WordNetLemmatizer，默认词性是verb\n",
    "'''\n",
    "def clean2(text):\n",
    "    #1.Pre-clean\n",
    "    text=preclean(text)\n",
    "    #2.toknenize\n",
    "#     text=tokenizer.tokenize(text)  #按空格和标点符号分\n",
    "    text_list=[word_tokenize(t) for t in sent_tokenize(text)] #先按句分，再按词分，还是会把人名的given name简写的.去掉（依然把它当句号）\n",
    "    print(text_list)\n",
    "    #remove one letter\n",
    "    text = [w for sent_list in text_list for w in sent_list if len(w)>=2]\n",
    "    print(text)\n",
    "    #3.remove stopwords\n",
    "    text = [w for w in text if not w in eng_stopwords] \n",
    "    #4.lemmatize,stem\n",
    "    text = [lem.lemmatize(word, \"v\") for word in text] \n",
    "    clean_text=\" \".join(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用SnowballStemmer；\n",
    "保留了单个字母的人名的given name\n",
    "'''\n",
    "def clean3(text):\n",
    "    #1.Pre-clean\n",
    "    text=preclean(text)\n",
    "    #2.toknenize\n",
    "    text_list=[word_tokenize(t) for t in sent_tokenize(text)] #先按句分，再按词分，还是会把人名的given name简写的.去掉\n",
    "#     print(text_list)\n",
    "    \n",
    "    #remove one letter\n",
    "#     text = [w for sent_list in text_list for w in sent_list if len(w)>=2]\n",
    "#     print(text)\n",
    "\n",
    "    #3.remove stopwords\n",
    "    text_list = [w for sent_list in text_list for w in sent_list if not w in eng_stopwords]\n",
    "#     print(text)\n",
    "    #4.stem\n",
    "    text_list = [stemmer.stem(word) for word in text_list] \n",
    "#     print(text)\n",
    "    text = retain_letter2(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用WordNetLemmatizer，默认词性是verb；\n",
    "保留了单个字母的人名的given name（含.）\n",
    "'''\n",
    "def clean4(text):\n",
    "    #1.Pre-clean\n",
    "    text=preclean(text)\n",
    "    #2.toknenize\n",
    "    text_list=[word_tokenize(t) for t in sent_tokenize(text)] #先按句分，再按词分\n",
    "#     print(text_list)\n",
    "    \n",
    "    #remove one letter\n",
    "#     text = [w for sent_list in text_list for w in sent_list if len(w)>=2]\n",
    "#     print(text)\n",
    "\n",
    "    #3.remove stopwords\n",
    "    text_list = [w for sent_list in text_list for w in sent_list if not w in eng_stopwords]\n",
    "#     print(text)\n",
    "    #4.stem\n",
    "    text_list = [lem.lemmatize(word, \"v\") for word in text_list] #list\n",
    "#     print(text)\n",
    "    text = retain_letter2(text_list)\n",
    "#     clean_text=\" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用WordNetLemmatizer, 使用pos_tag标注词性；\n",
    "保留了单个字母的人名的given name（含.）\n",
    "'''\n",
    "def clean5(text):\n",
    "    #1.Pre-clean\n",
    "    text=preclean(text)\n",
    "    #2.toknenize\n",
    "    text_list=[word_tokenize(t) for t in sent_tokenize(text)] #先按句分，再按词分，返回二维列表\n",
    "#     print(\"Toknenize:\",text_list)\n",
    " \n",
    "    #3.stem\n",
    "    res_list=[]\n",
    "    for t in text_list:\n",
    "        for word,pos in pos_tag(t):\n",
    "            wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "            res_list.append(lem.lemmatize(word, pos=wordnet_pos))\n",
    "#     print(\"Stem:\",res_list)\n",
    "    \n",
    "    #remove one letter\n",
    "#     text = [w for sent_list in text_list for w in sent_list if len(w)>=2]\n",
    "#     print(text)\n",
    "\n",
    "    #3.remove stopwords\n",
    "    text_list = [w for w in res_list if not w in eng_stopwords]\n",
    "#     print(\"Remove Stopwords:\",text_list)\n",
    "\n",
    "    text = retain_letter2(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用WordNetLemmatizer, 使用pos_tag标注词性；\n",
    "去掉了单个字母的人名的given name的简写（含.）\n",
    "'''\n",
    "def clean6(text):\n",
    "    #1.Pre-clean\n",
    "    text=preclean2(text)\n",
    "    #2.toknenize\n",
    "    text_list=[word_tokenize(t) for t in sent_tokenize(text)] #先按句分，再按词分\n",
    "#     print(\"Toknenize:\",text_list)\n",
    "\n",
    "    #remove one letter\n",
    "    text_list = [w for sent_list in text_list for w in sent_list if len(w)>=2]\n",
    "#     print(\"remove one letter:\",text_list)\n",
    " \n",
    "    #3.stem\n",
    "    res_list=[]\n",
    "    for word,pos in pos_tag(text_list):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res_list.append(lem.lemmatize(word, pos=wordnet_pos))\n",
    "#     print(\"Stem:\",res_list)\n",
    "    \n",
    "    #3.remove stopwords\n",
    "    text_list = [w for w in res_list if not w in eng_stopwords]\n",
    "#     print(\"Remove Stopwords:\",text_list)\n",
    "\n",
    "    text=\" \".join(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用WordNetLemmatizer, 使用pos_tag标注词性；\n",
    "去掉了单个字母的人名的given name的简写（含.）\n",
    "'''\n",
    "def clean7(text):\n",
    "    #1.Pre-clean\n",
    "    text=preclean(text)\n",
    "    #2.toknenize\n",
    "    text_list=[word_tokenize(t) for t in sent_tokenize(text)] #先按句分，再按词分\n",
    "#     print(\"Toknenize:\",text_list)\n",
    "\n",
    "    #remove one letter\n",
    "    text_list = [w for sent_list in text_list for w in sent_list if len(w)>=2]\n",
    "#     print(\"remove one letter:\",text_list)\n",
    " \n",
    "    #3.stem\n",
    "    res_list=[]\n",
    "    for word,pos in pos_tag(text_list):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res_list.append(lem.lemmatize(word, pos=wordnet_pos))\n",
    "#     print(\"Stem:\",res_list)\n",
    "    \n",
    "    #3.remove stopwords\n",
    "    text_list = [w for w in res_list if not w in eng_stopwords]\n",
    "#     print(\"Remove Stopwords:\",text_list)\n",
    "\n",
    "    text = retain_letter3(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C B a Special  \\r \\n characters  like jumping jumped dixit the spaces of D f eye  dictionaries said leaves tortures torturing right Rights'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'C. B. a Special [$#!^...]#~ \\r \\n characters. [$#!^ like jumping jumped dixit the spaces of D f eye 888323 dictionaries said leaves tortures torturing right Rights'\n",
    "retain_letter_space(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C. B. a Special [$#!^...]#~ characters [$#!^ like jumping jumped dixit the spaces of D f eye 888323 dictionaries said leaves'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retain_ascii(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C B a Special $^~ characters $^ like jumping jumped dixit the spaces of D f eye 888323 dictionaries said leaves'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_punctuation2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C B a Special  characters  like jumping jumped dixit the spaces of D f eye 888323 dictionaries said leaves'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_punctuation(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    a       b'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b='i ii iii iv a v vi vii viii ix x b'\n",
    "clean_romanumber(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c. b. a special [$#!^...]#~ characters [$#!^ like jumping jumped dixit the spaces of d f eye 888323 dictionaries said leaves'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preclean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'special character like jump jump dixit space eye dictionaries say leave'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['c.', 'b.', 'a', 'special', '[', '$', '#', '!', '^', '...', ']', '#', '~', 'characters', '[', '$', '#', '!', '^', 'like', 'jumping', 'jumped', 'dixit', 'the', 'spaces', 'of', 'd', 'f', 'eye', '888323', 'dictionaries', 'said', 'leaves']]\n",
      "['c.', 'b.', 'special', '...', 'characters', 'like', 'jumping', 'jumped', 'dixit', 'the', 'spaces', 'of', 'eye', '888323', 'dictionaries', 'said', 'leaves']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c. b. special ... character like jump jump dixit space eye 888323 dictionaries say leave'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c. b. special charact like jump jump dixit space eye dictionari said leav'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean3(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c. b. special character like jump jump dixit space eye dictionaries say leave'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean4(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c. b. special character like jump jump dixit space eye dictionary say leaf'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean5(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'special character like jump jump dixit space eye dictionary say leave torture torture right right'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean6(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toknenize: [['c.', 'b.', 'a', 'special', '[', '$', '#', '!', '^', '...', ']', '#', '~', 'characters', '.'], ['[', '$', '#', '!', '^', 'like', 'jumping', 'jumped', 'dixit', 'the', 'spaces', 'of', 'd', 'f', 'eye', '888323', 'dictionaries', 'said', 'leaves', 'tortures', 'torturing', 'right', 'rights']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'special character like jump jump dixit space eye dictionary say leave torture torture right right'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean7(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean6(doc_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean7(doc_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0\n",
      "i:1\n",
      "i:2\n",
      "i:3\n",
      "i:4\n",
      "i:5\n",
      "i:6\n",
      "i:7\n",
      "i:8\n",
      "i:9\n",
      "i:10\n",
      "i:11\n",
      "i:12\n",
      "i:13\n",
      "i:14\n",
      "i:15\n",
      "i:16\n",
      "i:17\n",
      "i:18\n",
      "i:19\n",
      "i:20\n",
      "i:21\n",
      "i:22\n",
      "i:23\n"
     ]
    }
   ],
   "source": [
    "fd_store_dir=os.path.join(cwd,\"clean_output3\")\n",
    "if(not os.path.exists(fd_store_dir)):\n",
    "        os.mkdir(fd_store_dir)\n",
    "for i in range(len(doc_list)):\n",
    "# for i in range(2):\n",
    "    print(\"i:\"+str(i))\n",
    "   \n",
    "    name=filenames[i]\n",
    "    name2=name.split(\".\")[0]\n",
    "    file_store_path=os.path.join(fd_store_dir,name2+\".txt\")\n",
    "\n",
    "    text=clean7(doc_list[i])\n",
    "    write_file(file_store_path,text)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
