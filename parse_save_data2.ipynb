{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#parser\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#NLP\n",
    "import string\n",
    "from autocorrect import spell\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\Jupyter Notebook\\COMP6237_Data_Mining\\input2\\gap-html\n",
      "['gap_-C0BAAAAQAAJ', 'gap_2X5KAAAAYAAJ', 'gap_9ksIAAAAQAAJ', 'gap_aLcWAAAAQAAJ', 'gap_Bdw_AAAAYAAJ', 'gap_CnnUAAAAMAAJ', 'gap_CSEUAAAAYAAJ', 'gap_DhULAAAAYAAJ', 'gap_dIkBAAAAQAAJ', 'gap_DqQNAAAAYAAJ', 'gap_fnAMAAAAYAAJ', 'gap_GIt0HMhqjRgC', 'gap_IlUMAQAAMAAJ', 'gap_MEoWAAAAYAAJ', 'gap_m_6B1DkImIoC', 'gap_ogsNAAAAIAAJ', 'gap_pX5KAAAAYAAJ', 'gap_RqMNAAAAYAAJ', 'gap_TgpMAAAAYAAJ', 'gap_udEIAAAAQAAJ', 'gap_VPENAAAAQAAJ', 'gap_WORMAAAAYAAJ', 'gap_XmqHlMECi6kC', 'gap_y-AvAAAAYAAJ']\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# 有效代码\n",
    "cwd=os.getcwd()\n",
    "fd_str=os.path.join(cwd,\"input2\\\\gap-html\")\n",
    "print(fd_str)\n",
    "# Get all dirs\n",
    "dirnames = [name for name in os.listdir(fd_str)\n",
    "        if os.path.isdir(os.path.join(fd_str, name))]\n",
    "print(dirnames)\n",
    "\n",
    "# Get all regular files\n",
    "allfiles_names=[]\n",
    "for dirname in dirnames:\n",
    "    dirpath=os.path.join(fd_str,dirname)\n",
    "    names = [os.path.join(dirpath, name) for name in os.listdir(dirpath) \n",
    "            if name.endswith('.html') and os.path.isfile(os.path.join(dirpath, name))]\n",
    "    allfiles_names.append(names)  #二维列表\n",
    "print(len(allfiles_names)) \n",
    "# print(allfiles_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效代码\n",
    "def read_file(file_path):\n",
    "#     with open(file_path, 'rt',encoding='latin-1') as f:\n",
    "    with open(file_path, 'rt',encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效代码\n",
    "def write_file(file_path,text):\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效代码\n",
    "def extract_text_by_bs(doc_text):\n",
    "    \"\"\"提取html页面的所有文本信息。\n",
    "    参考：\n",
    "    http://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python\n",
    "    \"\"\"\n",
    "    title, text = '', ''\n",
    "    soup = BeautifulSoup(doc_text, 'lxml')\n",
    "    try:\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            # get text\n",
    "            title = soup.title.string\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            pass\n",
    "        text = soup.get_text()\n",
    "        print(text)\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        print(\"Lines:\")\n",
    "#         lines = (print(line.strip()) for line in text.splitlines())\n",
    "        for line in text.splitlines():\n",
    "            print(line+\"\\r\\n\")\n",
    "        \n",
    "        print(\"Chunks:\")\n",
    "        for line in lines:\n",
    "            for phrase in line.split(\"  \"):\n",
    "                print(phrase)\n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_by_bs2(doc_text):\n",
    "    title, text = '', ''\n",
    "    soup = BeautifulSoup(doc_text, 'lxml')\n",
    "    spans_lines=soup.find_all(\"span\",\"ocr_line\")\n",
    "    spans_cinfo=soup.find_all(\"span\",\"ocr_cinfo\")\n",
    "#     print(spans_line)\n",
    "#     for span_line in spans_lines:\n",
    "#         print(span_line.get_text())\n",
    "    text_list=[span_line.get_text() for span_line in spans_lines]\n",
    "    text='\\r\\n'.join(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "将一行的末单词和下一行的首单词能组成词的放在了下一行首，麻烦，放在该行的末尾简单\n",
    "'''\n",
    "def extract_text_by_bs3(doc_text):\n",
    "    title, text = '', ''\n",
    "    soup = BeautifulSoup(doc_text, 'lxml')\n",
    "    spans_lines=soup.find_all(\"span\",\"ocr_line\")\n",
    "#     print(spans_lines)\n",
    "    if(spans_lines==[]):\n",
    "        return text\n",
    "    \n",
    "    prev_word=''\n",
    "    sent_prev_line=''\n",
    "    spans_cinfo_text_list_prev=[]\n",
    "    i=0\n",
    "    for span_line in spans_lines:\n",
    "#         print(span_line.get_text())\n",
    "        spans_cinfos=span_line.find_all(\"span\",\"ocr_cinfo\")\n",
    "#         print(spans_cinfos)\n",
    "        if(spans_cinfos==[]):\n",
    "            continue\n",
    "            \n",
    "        spans_cinfo_text_list=[span.get_text() for span in spans_cinfos]\n",
    "#         print(str(i)+':',spans_cinfo_text_list)\n",
    "\n",
    "        spans_cinfo_text_list1=spans_cinfo_text_list[1:len(spans_cinfo_text_list)-1] #去除第一个和最后一个词\n",
    "   \n",
    "        sent_line=' '.join(spans_cinfo_text_list1)\n",
    "        first_word=spans_cinfo_text_list[0]\n",
    "        combine_word=prev_word+first_word\n",
    "        if(wordnet.synsets(combine_word)!=[] and wordnet.synsets(first_word)==[]): #一行的末单词和下一行的首单词构成一个词\n",
    "#       下一行的首单词不是一个词   \n",
    "            sent_line=combine_word+' '+sent_line\n",
    "        else:\n",
    "            sent_line=first_word+' '+sent_line\n",
    "            if(i>0): #第一行不用\n",
    "                if(len(spans_cinfo_text_list_prev)>1):\n",
    "                    sent_prev_line+=' '+spans_cinfo_text_list_prev[len(spans_cinfo_text_list_prev)-1]  #前一行加入最后一个词\n",
    "#                 print(str(i)+'prev:',sent_prev_line)\n",
    "        \n",
    "#         print(str(i)+'cur:',sent_line)\n",
    "        \n",
    "        text+=sent_prev_line+'\\r'+'\\n'  \n",
    "#         print(text)\n",
    "        \n",
    "        last_word=spans_cinfo_text_list[len(spans_cinfo_text_list)-1]\n",
    "        prev_word=last_word\n",
    "        sent_prev_line=sent_line\n",
    "        spans_cinfo_text_list_prev=spans_cinfo_text_list\n",
    "       \n",
    "        i=i+1\n",
    "        \n",
    "    if(len(spans_cinfo_text_list_prev)>1):\n",
    "        text+=sent_line+' '+spans_cinfo_text_list[len(spans_cinfo_text_list)-1]+'\\r'+'\\n' \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\Jupyter Notebook\\COMP6237_Data_Mining\\input2\\gap-html\\gap_9ksIAAAAQAAJ\\00000009.html\n"
     ]
    }
   ],
   "source": [
    "name='gap_9ksIAAAAQAAJ\\\\00000009.html'\n",
    "file_path = os.path.join(fd_str, name)\n",
    "print(file_path)\n",
    "file_data=read_file(file_path) \n",
    "# titel,text=extract_text_by_bs(file_data)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=extract_text_by_bs2(file_data)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\Jupyter Notebook\\COMP6237_Data_Mining\\input2\\gap-html\\gap_9ksIAAAAQAAJ\\00000009.html\n",
      "\n",
      "Contents of the Second Volume.\n",
      "CONTENTS of Book V,\n",
      "YEAR X. The truce endeth. Cleon sent commander Into\n",
      "Thrace; his proceedings there. The battle of Arophipolis,\n",
      "in which Brasidas and Cleon are killed. A general peace, Ailed\n",
      "the Nician. An alliance, offensive and defensive, between the\n",
      "Athenians and Lacedæmonians. — — P, I\n",
      "Year Xs. The peace merely nominal; and Thucydides proceedeth\n",
      "in his history of the Peloponnesian war. The Corinthians\n",
      "practise against the Lacedæmonians. An Argive league. No\n",
      "confidence between the principal States. A train of negotiations,\n",
      "A separate alliance between the Lacedæmonians and Bœotians,\n",
      "contrary to article. Panactum demolished. — P. 23\n",
      "Year XII, The demolition of Panactum and the separate alliance\n",
      "highly resented'at Athens. Many are scheming a rupture, but\n",
      "especially Alcibiades. By his means a negotiation is brought on\n",
      "at Athens, and an alliance formed with the Argives. The La\n",
      "cedæmonians forbidden to assist at the Olympic games. P. 36\n",
      "Year XIII. War between the Argives and Epidaurians. The La\n",
      "cedæmonians throw a garrison into Epidaurus; and the Atheni\n",
      "ans replace the Helots and Messenians in Pylus. P. 51\n",
      "Year XIV. The Lacedæmonians take the field against the Argives.\n",
      "Two large Arm/es face one another within sight of Argos, yet\n",
      "part without engaging. The Lacedæmonians take the field a\n",
      "second time. The battle of Mantinea, The Argives enter into\n",
      "leagu* with the Lacedæmonians. — — ^* 54-\n",
      "Year XV. Fresh stirs at Argos in favour of the Athenians. P. 74.\n",
      "Year XVI. Expedition of the Athenians against the isli of Melos.\n",
      "The conserence in form, by way of dialogue. The Athenians\n",
      "become masters of that island. — — P. 76\n",
      "CONTENTS of Book VI.\n",
      "The Athenians resolve on the expedition to Sicily. Description of\n",
      "that island. — — — — P. 92\n",
      "Year XVII. The debate in the assembly of the people at Athens a-\n",
      "bout the expedition. The generals nominated with full powers.\n",
      "The affair of the Mercuries. Departure of the grand fleet for Si\n",
      "cily. Proceedingsat Syracuse. The Athenian fleet arrives on\n",
      "the coast of Italy. Alcibiades recalled, to take his trial about\n",
      "the Mercuries and profanation of the Mysteries. A digression,—\n",
      "containing the true account of a former revolution at Athens, be\n",
      "gun by Harmodius and Aristogiton. Alcibiades flies, and is\n",
      "proclaimed a traitor. . The Athenians land at Syracuse. A battle\n",
      "ensues, in which the Athenians are victorious; but, soon after,\n",
      "they return to Catana. The negotiations at Camarina. Alcibi\n",
      "ades at Sparta. — — — P. 99\n",
      "Year XVIII. The Athenians land again at Syracuse, take Epipolæ\n",
      "by surprize, and begin to invest Syracuse in form. Battles: The\n",
      "Athenians carry on their works; counterwork of the Syracusans.\n",
      "Aid sent to Syracuse from Peloponnesus, under the command of\n",
      ". . . Gylippus;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name='gap_9ksIAAAAQAAJ\\\\00000009.html'\n",
    "file_path = os.path.join(fd_str, name)\n",
    "print(file_path)\n",
    "file_data=read_file(file_path) \n",
    "text=extract_text_by_bs3(file_data)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0\n",
      "0, \n",
      "1,  800020427L \n",
      "  \n",
      "► \n",
      "  \n",
      "ZOI5  ZOI5\n",
      "2,  800020427L \n",
      "  \n",
      "► \n",
      "  \n",
      "ZOI5  ZOI5 DICTIONARY \n",
      "GREEK AND ROMAN GEOGRAPHY.\n",
      "  \n",
      "ns\n",
      "i:1\n",
      "0, \n",
      "1,  CReSlorecl  through\n",
      "a arant in\n",
      "from \n",
      "( \n",
      "The Cartwright Foundation 5\n",
      "( \n",
      "PRINCETON \n",
      "UNIVERSITY \n",
      "K LIBRARY J\n",
      "2,  CReSlorecl  through\n",
      "a arant in\n",
      "from \n",
      "( \n",
      "The Cartwright Foundation 5\n",
      "( \n",
      "PRINCETON \n",
      "UNIVERSITY \n",
      "K LIBRARY J \n",
      "i:2\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:3\n",
      "0, \n",
      "1,  \n",
      "2,   4 s3<? • (\n",
      "i:4\n",
      "0, University of Virginia Library\n",
      "DG207 L5 B3 1797 V.1\n",
      "ALD The history ot Rome.__\n",
      "II ii II ill ii mi IJIIM imuii\n",
      "UX DD1 132 Sit,\n",
      "1, University of Virginia Library\n",
      "DG207 L5 B3 1797 V.1\n",
      "ALD The history ot Rome.__\n",
      "II ii II ill ii mi IJIIM imuii\n",
      "UX DD1 132 Sit, \n",
      "2, University of Virginia Library\n",
      "DG207 L5 B3 1797 V.1\n",
      "ALD The history ot Rome.__\n",
      "II ii II ill ii mi IJIIM imuii\n",
      "UX DD1 132 Sit,  \n",
      "i:5\n",
      "0, \n",
      "1,  Univcr?  '-\n",
      ".-..3 \n",
      "General Library System\n",
      "University of Wisconsin - Madison\n",
      "728 State Street\n",
      "Madison, Wl 53706-1494\n",
      "U.S.A.\n",
      "2,  Univcr?  '-\n",
      ".-..3 \n",
      "General Library System\n",
      "University of Wisconsin - Madison\n",
      "728 State Street\n",
      "Madison, Wl 53706-1494\n",
      "U.S.A. \n",
      "i:6\n",
      "0, \n",
      "1,  K'E  ^\n",
      "2,  K'E  ^ \n",
      "i:7\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:8\n",
      "0, a&lral BMBBIB ::l\n",
      "  \n",
      "- :-,.-.-----. - j. - ' - - :. .\n",
      ",',-'.'.','.'.'\"---.\".'.'•- \n",
      "'u-\"\". -: :- : --'•'.--'•-- -\n",
      "1, a&lral BMBBIB ::l\n",
      "  \n",
      "- :-,.-.-----. - j. - ' - - :. .\n",
      ",',-'.'.','.'.'\"---.\".'.'•- \n",
      "'u-\"\". -: :- : --'•'.--'•-- - 600074686 \n",
      "  \n",
      "  \n",
      "/ \n",
      "121 f //\n",
      "2, a&lral BMBBIB ::l\n",
      "  \n",
      "- :-,.-.-----. - j. - ' - - :. .\n",
      ",',-'.'.','.'.'\"---.\".'.'•- \n",
      "'u-\"\". -: :- : --'•'.--'•-- - 600074686 \n",
      "  \n",
      "  \n",
      "/ \n",
      "121 f // \n",
      "i:9\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:10\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:11\n",
      "0, -A \n",
      "  \n",
      ",  Pi\n",
      "  \n",
      "%»\n",
      "1, -A \n",
      "  \n",
      ",  Pi\n",
      "  \n",
      "%» 600010679T \n",
      "  \n",
      "16.3U \n",
      "•I: \n",
      "\\\n",
      "2, -A \n",
      "  \n",
      ",  Pi\n",
      "  \n",
      "%» 600010679T \n",
      "  \n",
      "16.3U \n",
      "•I: \n",
      "\\ .- \n",
      "/\"  /\"\n",
      "i:12\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:13\n",
      "0, \n",
      "1,  ^ ^\"^\"^\"^ *$\"^'^\"^*\n",
      "Harvard  College\n",
      "Library \n",
      "  \n",
      "FROM THE BEQUEST OF\n",
      "JOHN HARVEY TREAT\n",
      "OF LAWRENCE, MASS.\n",
      "CLASS OF 1862\n",
      "2,  ^ ^\"^\"^\"^ *$\"^'^\"^*\n",
      "Harvard  College\n",
      "Library \n",
      "  \n",
      "FROM THE BEQUEST OF\n",
      "JOHN HARVEY TREAT\n",
      "OF LAWRENCE, MASS.\n",
      "CLASS OF 1862 \n",
      "i:14\n",
      "0, \n",
      "1,  H/ \n",
      "COLLEGE \n",
      "LIBRARY\n",
      "2,  H/ \n",
      "COLLEGE \n",
      "LIBRARY \n",
      "i:15\n",
      "0, \n",
      "1,  ►  ►\n",
      "2,  ►  ► \n",
      "i:16\n",
      "0, \n",
      "1,  ifilrrarg  uf\n",
      "tyxmteian  Httiir^r»itgt.\n",
      "2,  ifilrrarg  uf\n",
      "tyxmteian  Httiir^r»itgt. \n",
      "i:17\n",
      "0, \n",
      "1,  |_-£/6,4./f  ./a\n",
      "  \n",
      "  \n",
      "  \n",
      "HARVARD  COLLEGE\n",
      "LIBRARY \n",
      ",MrprNTNt  Thayer.\n",
      "2,  |_-£/6,4./f  ./a\n",
      "  \n",
      "  \n",
      "  \n",
      "HARVARD  COLLEGE\n",
      "LIBRARY \n",
      ",MrprNTNt  Thayer. \n",
      "i:18\n",
      "0, S^sa.  S^sa.\n",
      "1, S^sa.  S^sa. .*\n",
      "2, S^sa.  S^sa. .* y \n",
      "E  E\n",
      "i:19\n",
      "0, \n",
      "1,  1  1\n",
      "2,  1  1 \n",
      "i:20\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:21\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:22\n",
      "0, \n",
      "1,  \n",
      "2,   \n",
      "i:23\n",
      "0, iiiiiii  :\n",
      "HX IHT3 B\n",
      "1, iiiiiii  :\n",
      "HX IHT3 B ^arbart College Htbrarg.\n",
      "THE PARKMAN COLLECTION.\n",
      "BEQUEATHED  BY\n",
      "Francis f* ar k m a n ,\n",
      "(H.C.  1844).\n",
      "Received January /'. rflgt.\n",
      "2, iiiiiii  :\n",
      "HX IHT3 B ^arbart College Htbrarg.\n",
      "THE PARKMAN COLLECTION.\n",
      "BEQUEATHED  BY\n",
      "Francis f* ar k m a n ,\n",
      "(H.C.  1844).\n",
      "Received January /'. rflgt. Pa \n",
      "- \n",
      "ssa  ssa\n"
     ]
    }
   ],
   "source": [
    "# 有效代码\n",
    "fd_store=os.path.join(cwd,\"output3\")\n",
    "if(not os.path.exists(fd_store)):\n",
    "    os.mkdir(fd_store)\n",
    "for i in range(len(allfiles_names)):\n",
    "# for i in range(2):\n",
    "    print(\"i:\"+str(i))\n",
    "    text_combine=''\n",
    "    j=0\n",
    "    for file_path in allfiles_names[i]:\n",
    "        path_list=file_path.split('\\\\')\n",
    "        dir_name=path_list[len(path_list)-2]\n",
    "        fd_store_dir=os.path.join(fd_store, dir_name)\n",
    "        if(not os.path.exists(fd_store_dir)):\n",
    "            os.mkdir(fd_store_dir)\n",
    "        name=path_list[len(path_list)-1]\n",
    "        name2=name.split(\".\")[0]\n",
    "        file_store_path=os.path.join(fd_store_dir,name2+\".txt\")\n",
    "        \n",
    "        file_data=read_file(file_path) \n",
    "        text=extract_text_by_bs3(file_data)\n",
    "        write_file(file_store_path,text)  #every file parsed from html files save to the folder\n",
    "        \n",
    "        text_combine = text_combine+\" \"+text.strip()\n",
    "        \n",
    "        if(j<3):\n",
    "            print(str(j)+\",\"+text_combine)\n",
    "        j=j+1\n",
    "        \n",
    "    file_com_path=os.path.join(fd_store,dir_name+\".txt\") #\n",
    "    write_file(file_com_path,text_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\Jupyter Notebook\\COMP6237_Data_Mining\\input2\\gap-html\\gap_2X5KAAAAYAAJ\\00000002.html\n",
      "\n",
      "CReSlorecl  through\n",
      "a arant in\n",
      "from \n",
      "( \n",
      "The Cartwright Foundation 5\n",
      "( \n",
      "PRINCETON \n",
      "UNIVERSITY \n",
      "K LIBRARY J\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name='gap_2X5KAAAAYAAJ\\\\00000002.html' \n",
    "file_path = os.path.join(fd_str, name)\n",
    "print(file_path)\n",
    "file_data=read_file(file_path) \n",
    "text=extract_text_by_bs3(file_data)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
